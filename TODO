- Add NaN and Inf catching to key places in code
- Make BBoxes start with min +inf and max -inf by default
- Track down bug where single rays from each round hit something even
  when there's nothing to hit.

- Eliminate geometry cache:
	- All tracing API's should accept collections of rays

- Investigate LRU caches that are lock-free for reading

- Two-stage ray tracing, similar to the Rayes (not Reyes) paper
	//- Make rays const throughout the tracing pipeline.
	//- All mutable data about tracing a ray are stored in intersections.
	- Assign each thread an object id to trace rays for.
		- Multiple threads can be tracing the same ray against different
		  objects, so how to update intersections in a thread-safe way
		  without too much contention between threads or threads waiting
		  on each other?  Ideally we also want to update intersections
		  as early as possible, to minimize unnecessary tracing work
		  when a hit has already been found.  Really need to work out
		  a solid design here.
		- Maybe access to the shared intersection data is like a
		  revolving door, where when a thread is ready to write its
		  intersections, it gets a lock and writes, but also immediately
		  does the reading necessary for the next round of ray tests
		  before releasing the lock.  That way the wait time is strictly
		  limited to the read/write time of the shared data.  And if
		  we're able to keep intersection data very small, that
		  read/write time will also be small.
	- Refactor data structures and API to pass around/duplicate minimal
	  data when handling ray reordering and ray tracing.

- MicroSurface should calculate normal and point differentials for
  ray intersections.
	- What is the minimum amount of data we need to store to calculate
	  this efficiently?
		- We can assume grids for now.  If we need to lift that
		  assumption in the future, we can deal with that then.
		- Goal is to keep things compact.  How much can we quantize
		  things, especially if we limit grid size?
			- UV's are always regular, so we can quantize them without
			  it even being lossy.
			- Quantizing normals might work if we still interpolate
			  in unquantized values.
		- Is there some innovative way to calculate good-enough
		  approximations based on the bounding boxes themselves?

- Investigate ray differential corner-cases:
	- Rays that are nearly tangent to the surfaces they intersect
	- Given that surfaces are approximated with bounding boxes,
	  chances of a ray hitting a surface at tangent angle is
	  non-zero.  That should be handled somehow.
		- Probabilistic misses based on incident angle...?  Would likely
		  creating tunneling--how to handle that?  Would it be a problem
		  in practice?

- Implement a basic shading system that does proper ray differential
  propigation, so that the code for that is all in one convenient
  place.
	- A differential geometry class would be very useful for this.

- Framework for surface primitives to manage their splitting and micropolygons
  in a thread-safe way.
	- Should be generic enough for all surface primitives to use.
	- Perhaps define an API that primitives must expose, which the framework
	  can use.
	- How can this relate to a bigger picture that handles, e.g.,
	  particles, curves, and volumes as well?

- Surface primitive API:
	- separate() -- separates out natural component primitives that
	                together make up the original.  Used on scene
	                loading to make sure everything can be handled
	                by the surface framework.
	- split() -- splits the primitive into multiple primitives that
	             together make up the original.  May be called on scene
	             loading, but is primarily for the tracing stages.
	- micro_estimate() -- given a target microelement size, estimates
	                      the number of microelements that would have to
	                      be generated to achieve it.
	- micro_generate() -- generates the MicroSurface





- Migrate all render configuration data to Renderer

- Simple scene-description format

- Adaptive sampling scheme
	//- Calculate variance information per pixel
	- Reduce the Sampler's responsibilities, migrating pixel/sample
	  traversal to be the Integrator's responsibilitiy.
	- Implement adaptive sample traversal based on variance

- Refactor TimeBox template to use class methods for interpolation

- Multi-threaded rendering

- OSL integration

- More compact and/or faster top-level BVH:
	- More compact probably actually doesn't matter so much, given
	  the intended usage of Psychopath.  The top-level BVH is likely
	  to be relatively sparse anyway compared to most path tracers,
	  since it uses higher-order surface descriptions.  Focus on speed.
	- Four basic approaches to speeding up traversal:
		- Reducing the number of nodes traversed on average
		- Reducing the cost of testing against each node
		- Using SIMD to test multiple nodes at once
		- Improving cache coherency of traversal by better tree layout
	- Must be able to pause and resume traversal with minimal data per ray
	- QBVH is worth investigating, but it's unclear how to do efficient
	  resumes.
	- MSBVH is good to look at, especially if some primitives may be
	  very large and clog up the tree.  Also could just try to keep large
	  primitives higher in the tree.
	- Single Slab Hierarchy is very interesting, but it's unclear if
	  efficient resumes are possible.  Unless we actually keep the
	  entire BBoxes at each node, and only trace it like a SSH when
	  possible...?  Maybe generalize to marking specific BBox sides
	  as important.
	- Variable-child BVH could be interesting, with SIMD used where applicable.
	  Similar tricky resume issue as QBVH, though.
	- "Restart trail" approach for resuming might extend to higher 
	  branch factors, e.g. QBVH.
	- What performance impact does a fixed traversal order have?
	  Does the reduction in branching compensate for less efficient
	  traversal?  What is the most efficient fixed traversal order?
	  SAH-based?  Pre-randomized?
	  A fixed traversal order would make resumes trivial on almost
	  any tree structure.

//- Faster sorting of potential ray intersections
	//- Counting sort
		- Big benefit of counting sort is we automatically get the starting
		  indices in the potential intersections array for each object.
		- http://www.drdobbs.com/architecture-and-design/parallel-counting-sort/224700144
		- http://www.drdobbs.com/parallel/parallel-counting-sort-part-2/225900071

